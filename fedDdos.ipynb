{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        print(f\"Reading {file}\")\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        df_list.append(df)\n",
    "    data = pd.concat(df_list, ignore_index=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Drop columns with all zero values or NaNs\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Drop irrelevant columns if any\n",
    "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = data.drop('Label', axis=1)\n",
    "    y = data['Label']\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Normalize numerical features\n",
    "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "    return X, y_encoded, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(X, y, label_encoder, num_companies=5):\n",
    "    # Get unique attack types\n",
    "    attack_types = label_encoder.classes_\n",
    "\n",
    "    # Shuffle attack types\n",
    "    np.random.shuffle(attack_types)\n",
    "\n",
    "    # Assign attack types to companies\n",
    "    company_attack_types = {\n",
    "        f'Company_{i+1}': attack_types[i::num_companies] for i in range(num_companies)\n",
    "    }\n",
    "\n",
    "    # Split data for each company\n",
    "    company_data = {}\n",
    "    for company, attacks in company_attack_types.items():\n",
    "        # Get indices of the selected attack types\n",
    "        indices = [i for i, label in enumerate(y) if label_encoder.inverse_transform([label])[0] in attacks]\n",
    "        X_company = X.iloc[indices]\n",
    "        y_company = y[indices]\n",
    "        company_data[company] = (X_company, y_company)\n",
    "\n",
    "    return company_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_company_data(company_data, output_dir='company_data'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for company, (X_company, y_company) in company_data.items():\n",
    "        company_dir = os.path.join(output_dir, company)\n",
    "        os.makedirs(company_dir, exist_ok=True)\n",
    "        X_company.to_csv(os.path.join(company_dir, 'X.csv'), index=False)\n",
    "        np.save(os.path.join(company_dir, 'y.npy'), y_company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_dir = '/path/to/CICDDoS2019/CSV/'  # Update this path\n",
    "    data = load_data(data_dir)\n",
    "    X, y_encoded, label_encoder = preprocess_data(data)\n",
    "    company_data = partition_data(X, y_encoded, label_encoder, num_companies=5)\n",
    "    save_company_data(company_data)\n",
    "    # Save the label encoder for later use\n",
    "    import joblib\n",
    "    joblib.dump(label_encoder, 'label_encoder.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DDoSDetectionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DDoSDetectionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Personalization layer\n",
    "        self.personal_layer = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Personalization layer\n",
    "        x = F.relu(self.personal_layer(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "from model import DDoSDetectionModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.float())\n",
    "        loss = criterion(outputs, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data.float())\n",
    "            loss = criterion(outputs, target.long())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            loss_total += loss.item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, loss_total / len(test_loader)\n",
    "\n",
    "# Flower client\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, train_loader, test_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        # Exclude personal layers from being sent to the server\n",
    "        parameters = []\n",
    "        for name, param in self.model.state_dict().items():\n",
    "            if 'personal_layer' not in name:\n",
    "                parameters.append(param.cpu().numpy())\n",
    "        return parameters\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        # Update only the global layers\n",
    "        state_dict = self.model.state_dict()\n",
    "        global_layers = [name for name in state_dict.keys() if 'personal_layer' not in name]\n",
    "        params_dict = zip(global_layers, parameters)\n",
    "        for k, v in params_dict:\n",
    "            state_dict[k] = torch.tensor(v)\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        train(self.model, self.train_loader, self.criterion, self.optimizer, self.device)\n",
    "        return self.get_parameters(), len(self.train_loader.dataset), {}\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        accuracy, loss = test(self.model, self.test_loader, self.criterion, self.device)\n",
    "        return float(loss), len(self.test_loader.dataset), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "def load_data(company_dir):\n",
    "    X = pd.read_csv(os.path.join(company_dir, 'X.csv'))\n",
    "    y = np.load(os.path.join(company_dir, 'y.npy'))\n",
    "    return X, y\n",
    "\n",
    "def start_client(company_name):\n",
    "    # Load data\n",
    "    company_dir = os.path.join('company_data', company_name)\n",
    "    X, y = load_data(company_dir)\n",
    "\n",
    "    # Split into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Load label encoder\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = len(label_encoder.classes_)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DDoSDetectionModel(input_dim, output_dim)\n",
    "    \n",
    "    # Start Flower client\n",
    "    client = FLClient(model, train_loader, test_loader, device)\n",
    "    fl.client.start_numpy_client(server_address=\"localhost:8080\", client=client)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    company_name = sys.argv[1]  # Pass company name as command-line argument\n",
    "    start_client(company_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server.py\n",
    "import flwr as fl\n",
    "import torch\n",
    "from model import DDoSDetectionModel\n",
    "import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load label encoder to get input/output dimensions\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "    input_dim = 80  # Update with actual number of features\n",
    "    output_dim = len(label_encoder.classes_)\n",
    "    \n",
    "    # Initialize global model\n",
    "    model = DDoSDetectionModel(input_dim, output_dim)\n",
    "    initial_parameters = [val.cpu().numpy() for _, val in model.state_dict().items() if 'personal_layer' not in _]\n",
    "    \n",
    "    # Define strategy\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=1.0,\n",
    "        min_fit_clients=5,\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=5,\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(initial_parameters)\n",
    "    )\n",
    "    \n",
    "    # Start Flower server\n",
    "    fl.server.start_server(server_address=\"localhost:8080\", strategy=strategy, config={\"num_rounds\": 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How TO Run it:\n",
    "# In Terminal 1\n",
    "python server.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Terminals 2-6 (one for each company)\n",
    "python client.py Company_1\n",
    "python client.py Company_2\n",
    "python client.py Company_3\n",
    "python client.py Company_4\n",
    "python client.py Company_5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_evaluation.py\n",
    "import torch\n",
    "from model import DDoSDetectionModel\n",
    "from client import load_data\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "def evaluate_global_model():\n",
    "    # Load label encoder\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "    input_dim = 80  # Update with actual number of features\n",
    "    output_dim = len(label_encoder.classes_)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DDoSDetectionModel(input_dim, output_dim).to(device)\n",
    "    \n",
    "    # Load global model parameters\n",
    "    global_parameters = fl.common.parameters_to_ndarrays(strategy.parameters)\n",
    "    state_dict = model.state_dict()\n",
    "    global_layers = [name for name in state_dict.keys() if 'personal_layer' not in name]\n",
    "    params_dict = zip(global_layers, global_parameters)\n",
    "    for k, v in params_dict:\n",
    "        state_dict[k] = torch.tensor(v)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    # Load test data from all companies\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for company in ['Company_1', 'Company_2', 'Company_3', 'Company_4', 'Company_5']:\n",
    "        company_dir = os.path.join('company_data', company)\n",
    "        X, y = load_data(company_dir)\n",
    "        _, X_test, _, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
    "        X_list.append(X_test)\n",
    "        y_list.append(y_test)\n",
    "    X_test_global = np.concatenate(X_list)\n",
    "    y_test_global = np.concatenate(y_list)\n",
    "    \n",
    "    # Create test loader\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test_global), torch.tensor(y_test_global))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_scores.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "    print(report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_global_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
